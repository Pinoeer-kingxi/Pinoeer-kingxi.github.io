---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

{% capture intro_en %}
I am Weixi Lin, currently a Master's student in Computer Technology at Northwestern Polytechnical University. My research interests primarily focus on **large model optimization**, **RAG (Retrieval-Augmented Generation) systems**, and **AI inference acceleration**.

In academic and competition contexts, I actively participate in various AI innovation competitions, focusing on addressing performance bottlenecks of large models in practical applications. I have accumulated substantial practical experience, particularly in MoE (Mixture of Experts) model optimization and end-to-end RAG system design. Notably, in the MOE competition, I achieved a total score of 353.7 points (baseline 100), representing a 253.7% improvement.
{% endcapture %}

{% capture intro_zh %}
æˆ‘æ˜¯æ—ç‚œå¸Œï¼Œç›®å‰æ˜¯è¥¿åŒ—å·¥ä¸šå¤§å­¦è®¡ç®—æœºæŠ€æœ¯ä¸“ä¸šçš„åœ¨è¯»ç¡•å£«ç ”ç©¶ç”Ÿã€‚æˆ‘çš„ç ”ç©¶å…´è¶£ä¸»è¦é›†ä¸­åœ¨**å¤§æ¨¡å‹ä¼˜åŒ–**ã€**RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿ**å’Œ**AIæ¨ç†åŠ é€Ÿ**ç­‰é¢†åŸŸã€‚

åœ¨å­¦æœ¯å’Œç«èµ›æ–¹é¢ï¼Œæˆ‘ç§¯æå‚ä¸å„ç±»AIåˆ›æ–°å¤§èµ›ï¼Œä¸“æ³¨äºè§£å†³å¤§æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨MoEï¼ˆMixture of Expertsï¼‰æ¨¡å‹ä¼˜åŒ–å’Œå…¨é“¾è·¯RAGç³»ç»Ÿè®¾è®¡æ–¹é¢ç§¯ç´¯äº†ä¸°å¯Œçš„å®è·µç»éªŒã€‚åœ¨MOEæ¯”èµ›ä¸­ï¼Œæˆ‘å–å¾—äº†æ€»åˆ†353.7åˆ†ï¼ˆåŸºçº¿100åˆ†ï¼‰çš„æˆç»©ï¼Œæå‡äº†253.7%ã€‚
{% endcapture %}

<div data-lang="en" class="lang-content">
{{ intro_en | markdownify }}
</div>

<div data-lang="zh" class="lang-content" style="display: none;">
{{ intro_zh | markdownify }}
</div>

{% capture news_en %}
# ğŸ”¥ News
- *2025.10*: &nbsp;ğŸ‰ğŸ‰ End-to-end RAG solution design project ongoing, achieving 91% Hit Rate and 85%+ Factuality Score
- *2025.11*: &nbsp;ğŸ‰ğŸ‰ Ascend AI Innovation Competition - MindSpore Large Model Optimization project completed, Prefill performance improved by 266%, total score 353.7 (baseline 100, improvement of 253.7%)
- *2025.09*: &nbsp;ğŸ‰ğŸ‰ Won Silver Medal in MindSpore Model Development Challenge S1-MOE Track
{% endcapture %}

{% capture news_zh %}
# ğŸ”¥ News
- *2025.10*: &nbsp;ğŸ‰ğŸ‰ å…¨é“¾è·¯RAGæ–¹æ¡ˆè®¾è®¡é¡¹ç›®æŒç»­è¿›è¡Œä¸­ï¼Œå®ç°äº†91%çš„Hit Rateå’Œ85%+çš„Factuality Score
- *2025.11*: &nbsp;ğŸ‰ğŸ‰ æ˜‡è…¾AIåˆ›æ–°å¤§èµ› - MindSporeå¤§æ¨¡å‹ä¼˜åŒ–é¡¹ç›®å®Œæˆï¼ŒPrefillæ€§èƒ½æå‡266%ï¼Œæ€»åˆ†353.7ï¼ˆåŸºçº¿100ï¼Œæå‡253.7%ï¼‰
- *2025.09*: &nbsp;ğŸ‰ğŸ‰ è·å¾—æ˜‡æ€æ¨¡å‹å¼€å‘æŒ‘æˆ˜èµ› S1-MOEèµ›é“é“¶ç‰Œ
{% endcapture %}

<div data-lang="en" class="lang-content">
{{ news_en | markdownify }}
</div>

<div data-lang="zh" class="lang-content" style="display: none;">
{{ news_zh | markdownify }}
</div>

{% capture pub_en %}
# ğŸ“ Publications 

Currently, I have no published papers. Related work is in progress.
{% endcapture %}

{% capture pub_zh %}
# ğŸ“ Publications 

ç›®å‰æš‚æ— å·²å‘è¡¨çš„è®ºæ–‡ï¼Œç›¸å…³å·¥ä½œæ­£åœ¨è¿›è¡Œä¸­ã€‚
{% endcapture %}

<div data-lang="en" class="lang-content">
{{ pub_en | markdownify }}
</div>

<div data-lang="zh" class="lang-content" style="display: none;">
{{ pub_zh | markdownify }}
</div>

{% capture awards_en %}
# ğŸ– Honors and Awards
- *2025.09* Silver Medal in MindSpore Model Development Challenge S1-MOE Track
- *2025* CHIP2025 - Metabolic Disease Discharge Medication Recommendation Task for Chinese Electronic Medical Records Algorithm Competition (Rank: 17/573)
- *2025* Higress AI Gateway Development Challenge Third Prize
- *Undergraduate* National Scholarship, First-Class Excellence Scholarship
- *Undergraduate* First Prize in the National College Students Mathematical Modeling Competition (Undergraduate Division)
{% endcapture %}

{% capture awards_zh %}
# ğŸ– Honors and Awards
- *2025.09* æ˜‡æ€æ¨¡å‹å¼€å‘æŒ‘æˆ˜èµ› S1-MOEèµ›é“é“¶ç‰Œ
- *2025* CHIP2025-é¢å‘ä¸­æ–‡ç”µå­ç—…å†çš„ä»£è°¢æ€§ç–¾ç—…å‡ºé™¢ç”¨è¯æ¨èä»»åŠ¡_ç®—æ³•å¤§èµ›ï¼ˆæ’åï¼š17/573ï¼‰
- *2025* Higress AI ç½‘å…³å¼€å‘æŒ‘æˆ˜èµ› ä¸‰ç­‰å¥–
- *æœ¬ç§‘æœŸé—´* å›½å®¶å¥–å­¦é‡‘ã€ä¸€ç­‰ä¼˜ç§€å¥–å­¦é‡‘
- *æœ¬ç§‘æœŸé—´* æ•°ç»´æ¯å…¨å›½å¤§å­¦ç”Ÿæ•°å­¦å»ºæ¨¡ç«èµ›æœ¬ç§‘ç”Ÿç»„ä¸€ç­‰å¥–
{% endcapture %}

<div data-lang="en" class="lang-content">
{{ awards_en | markdownify }}
</div>

<div data-lang="zh" class="lang-content" style="display: none;">
{{ awards_zh | markdownify }}
</div>

{% capture edu_en %}
# ğŸ“– Education
- *2025.09 - Present*, Northwestern Polytechnical University, Computer Technology, Master's, School of Computer Science
- *2021.09 - 2025.07*, Shaanxi Normal University, Software Engineering, Bachelor's, School of Computer Science
{% endcapture %}

{% capture edu_zh %}
# ğŸ“– Educations
- *2025.09 - è‡³ä»Š*, è¥¿åŒ—å·¥ä¸šå¤§å­¦ï¼Œè®¡ç®—æœºæŠ€æœ¯ï¼Œç¡•å£«ï¼Œè®¡ç®—æœºå­¦é™¢
- *2021.09 - 2025.07*, é™•è¥¿å¸ˆèŒƒå¤§å­¦ï¼Œè½¯ä»¶å·¥ç¨‹ï¼Œæœ¬ç§‘ï¼Œè®¡ç®—æœºç§‘å­¦å­¦é™¢
{% endcapture %}

<div data-lang="en" class="lang-content">
{{ edu_en | markdownify }}
</div>

<div data-lang="zh" class="lang-content" style="display: none;">
{{ edu_zh | markdownify }}
</div>

<span class='anchor' id='projects'></span>

{% capture projects_en %}
# ğŸ’» Projects

## End-to-End RAG Solution Design â€” Integrating Dual Hybrid Retrieval, Adaptive Routing, and CRAG Dynamic Error Correction Mechanisms
*2025.10 - Present*

**Project Description**: Designed and developed a full-chain enhanced retrieval system based on MCP for complex scenarios in the Higress gateway. Addressed three major pain points of traditional RAG systems in long document retrieval: "version confusion", "severe hallucination", and "response latency".

**Core Responsibilities and Implementation**:
1. Built a layered architecture, including an MCP Server layer responsible for protocol routing, a RAG Client layer responsible for orchestration, and an infrastructure layer integrating Milvus and LLM
2. Introduced semantic caching mechanism to intercept high-frequency queries, achieving millisecond-level response (<50ms) for cache hits and significantly reducing Token consumption
3. Utilized Milvus's Boost Ranker functionality to implement business logic weighting (e.g., official documentation weight x1.2), combined with BGE-M3's sparse and dense vector dual-path recall, and fused through RRF algorithm, effectively solving the problem of old version document interference
4. Implemented Corrective RAG strategy, constructing a Retrieval Evaluator to score the confidence of retrieved documents. For "questionable" or "incorrect" retrieval results, automatically downgraded to trigger Tavily Web Search to supplement external information, building a highly reliable answer loop
5. Customized Higress Splitter for Markdown structure code block protection and hierarchical-aware segmentation; utilized Milvus Partition Key to achieve physical isolation of multi-version/multi-tenant data

**Project Results**:
- Hit Rate reached 91%, with Boost strategy for official documentation increasing hit rate from 60% to 80%
- Factuality Score exceeded 85%, significantly reducing large model hallucination generation
- Through dual verification of Semantic Score (66%) and MRR (0.577), demonstrated the system's superiority in complex long-tail questions

## Ascend AI Innovation Competition - MindSpore Large Model Optimization (MoE Track)
*2025.09 - 2025.11*

**Task**: Optimize the inference speed and memory footprint of DeepSeek-16b/Qwen-2.7B MoE models on Ascend NPU.

**Main Work**:
- **Bottleneck Analysis**: Identified that MoE module computation accounts for 95%+, establishing the optimization mainline of "trading space for time" and "serial to parallel"
- **Prefill Optimization**: Proposed Pad-BMM-Gather algorithm, utilizing tensor_scatter_update and batch matrix multiplication (ops.bmm) to replace native Python loops, achieving complete vectorization of expert computation, Prefill performance improved by 266%
- **Decode Optimization**: Designed dynamic weight caching mechanism, reorganizing discrete expert weights into continuous memory, optimizing memory access locality; combined with float32 mixed precision strategy, solved the precision overflow problem in parallel computation

**Results**: Final total score 353.7 (baseline 100, improvement of 253.7%), achieving multiple-fold improvement in inference speed while reducing peak memory by 17%.
{% endcapture %}

{% capture projects_zh %}
# ğŸ’» Projects

## å…¨é“¾è·¯ RAG æ–¹æ¡ˆè®¾è®¡ â€”â€” é›†æˆåŒé‡æ··åˆæ£€ç´¢ã€è‡ªé€‚åº”è·¯ç”±ä¸ CRAG åŠ¨æ€çº é”™æœºåˆ¶
*2025.10 - è‡³ä»Š*

**é¡¹ç›®æè¿°**: é’ˆå¯¹ Higress ç½‘å…³çš„å¤æ‚åœºæ™¯ï¼Œè®¾è®¡å¹¶å¼€å‘åŸºäºMCPçš„å…¨é“¾è·¯å¢å¼ºå‹æ£€ç´¢ç³»ç»Ÿã€‚è§£å†³äº†ä¼ ç»ŸRAGç³»ç»Ÿåœ¨é•¿æ–‡æ¡£æ£€ç´¢ä¸­å­˜åœ¨çš„"ç‰ˆæœ¬æ··æ·†"ã€"å¹»è§‰ä¸¥é‡"åŠ"å“åº”å»¶è¿Ÿ"ä¸‰å¤§ç—›ç‚¹ã€‚

**æ ¸å¿ƒèŒè´£ä¸å®æ–½**:
1. æ„å»ºåˆ†å±‚æ¶æ„ï¼ŒåŒ…å« MCP Server å±‚è´Ÿè´£åè®®è·¯ç”±ï¼ŒRAG Client å±‚è´Ÿè´£ç¼–æ’ï¼Œä»¥åŠåŸºç¡€è®¾æ–½å±‚é›†æˆ Milvus å’Œ LLM
2. å¼•å…¥è¯­ä¹‰ç¼“å­˜æœºåˆ¶ï¼Œæ‹¦æˆªé«˜é¢‘é—®é¢˜ï¼Œå®ç°ç¼“å­˜å‘½ä¸­è¯·æ±‚ <50ms çš„æ¯«ç§’çº§å“åº”ï¼Œå¤§å¹…é™ä½ Token æ¶ˆè€—
3. åˆ©ç”¨ Milvus çš„ Boost Ranker åŠŸèƒ½å®ç°ä¸šåŠ¡é€»è¾‘åŠ æƒï¼ˆå¦‚å®˜æ–¹æ–‡æ¡£æƒé‡ x1.2ï¼‰ï¼Œç»“åˆ BGE-M3 çš„ç¨€ç–ä¸ç¨ å¯†å‘é‡åŒè·¯å¬å›ï¼Œé€šè¿‡ RRF ç®—æ³•è¿›è¡Œèåˆï¼Œæœ‰æ•ˆè§£å†³æ—§ç‰ˆæœ¬æ–‡æ¡£å¹²æ‰°é—®é¢˜
4. å®ç° Corrective RAG ç­–ç•¥ï¼Œæ„å»º Retrieval Evaluator å¯¹å¬å›æ–‡æ¡£è¿›è¡Œç½®ä¿¡åº¦æ‰“åˆ†ã€‚é’ˆå¯¹"å­˜ç–‘"æˆ–"é”™è¯¯"çš„æ£€ç´¢ç»“æœï¼Œè‡ªåŠ¨é™çº§è§¦å‘ Tavily Web Search è¡¥å……å¤–éƒ¨ä¿¡æ¯ï¼Œæ„å»ºé«˜å¯é çš„å›ç­”é—­ç¯
5. å®šåˆ¶ Higress Splitterï¼Œé’ˆå¯¹ Markdown ç»“æ„è¿›è¡Œä»£ç å—ä¿æŠ¤ä¸å±‚çº§æ„ŸçŸ¥åˆ‡åˆ†ï¼›åˆ©ç”¨ Milvus Partition Key å®ç°å¤šç‰ˆæœ¬/å¤šç§Ÿæˆ·æ•°æ®çš„ç‰©ç†éš”ç¦»

**é¡¹ç›®æˆæœ**:
- Hit Rate è¾¾åˆ°91%ï¼Œå…¶ä¸­é’ˆå¯¹å®˜æ–¹æ–‡æ¡£çš„ Boost ç­–ç•¥ä½¿å‘½ä¸­ç‡ä» 60% æå‡è‡³ 80%
- Factuality Score è¶…è¿‡ 85%ï¼Œå¤§å¹…å‡å°‘äº†å¤§æ¨¡å‹çš„å¹»è§‰ç”Ÿæˆ
- é€šè¿‡ Semantic Score (66%) ä¸ MRR (0.577) çš„åŒé‡éªŒè¯ï¼Œè¯æ˜äº†ç³»ç»Ÿåœ¨å¤æ‚é•¿å°¾é—®é¢˜ä¸Šçš„ä¼˜è¶Šæ€§

## æ˜‡è…¾AIåˆ›æ–°å¤§èµ› - MindSporeå¤§æ¨¡å‹ä¼˜åŒ– (MoEèµ›é“)
*2025.09 - 2025.11*

**ä»»åŠ¡**: åœ¨ Ascend NPU ä¸Šä¼˜åŒ– DeepSeek-16b/Qwen-2.7B MoE æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ä¸æ˜¾å­˜å ç”¨ã€‚

**ä¸»è¦å·¥ä½œ**:
- **ç“¶é¢ˆåˆ†æ**: è¯†åˆ«å‡º MoE æ¨¡å—è®¡ç®—å æ¯”é«˜è¾¾ 95%+ï¼Œç¡®å®šäº†"ä»¥ç©ºé—´æ¢æ—¶é—´"å’Œ"ä¸²è¡Œè½¬å¹¶è¡Œ"çš„ä¼˜åŒ–ä¸»çº¿
- **Prefill ä¼˜åŒ–**: æå‡º Pad-BMM-Gather ç®—æ³•ï¼Œåˆ©ç”¨ tensor_scatter_update å’Œæ‰¹é‡çŸ©é˜µä¹˜æ³• (ops.bmm) æ›¿ä»£åŸç”Ÿ Python å¾ªç¯ï¼Œå®ç°ä¸“å®¶è®¡ç®—çš„å®Œå…¨å‘é‡åŒ–ï¼ŒPrefill æ€§èƒ½æå‡ 266%
- **Decode ä¼˜åŒ–**: è®¾è®¡åŠ¨æ€æƒé‡ç¼“å­˜æœºåˆ¶ï¼Œå°†ç¦»æ•£çš„ä¸“å®¶æƒé‡é‡ç»„ä¸ºè¿ç»­å†…å­˜ï¼Œä¼˜åŒ–æ˜¾å­˜è®¿é—®å±€éƒ¨æ€§ï¼›é…åˆ float32 æ··åˆç²¾åº¦ç­–ç•¥ï¼Œè§£å†³äº†å¹¶è¡Œè®¡ç®—çš„ç²¾åº¦æº¢å‡ºé—®é¢˜

**ç»“æœ**: æœ€ç»ˆæ€»åˆ† 353.7 (åŸºçº¿ 100ï¼Œæå‡ 253.7%)ï¼Œåœ¨é™ä½ 17% æ˜¾å­˜å³°å€¼çš„åŒæ—¶ï¼Œå®ç°äº†æ¨ç†é€Ÿåº¦çš„å€æ•°çº§è·ƒå‡ã€‚
{% endcapture %}

<div data-lang="en" class="lang-content">
{{ projects_en | markdownify }}
</div>

<div data-lang="zh" class="lang-content" style="display: none;">
{{ projects_zh | markdownify }}
</div>
